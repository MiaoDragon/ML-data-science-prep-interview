{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c065f817",
   "metadata": {},
   "source": [
    "This notebook goes over the basics of neural networks through implementing a multi-layered fully-connected neural network from scratch.\n",
    "\n",
    "Things to cover:\n",
    "- automatic gradient computation\n",
    "- vectorization and matrix multiplication\n",
    "- back-propagation\n",
    "- training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "224934ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e825a44",
   "metadata": {},
   "source": [
    "define the building blocks of neural network:\n",
    "- linear fully-connected layers\n",
    "- loss computation\n",
    "\n",
    "each layer and loss component implement the forward propagation step and gradient computation for back propagation.\n",
    "\n",
    "#### back-propagation\n",
    "chain rule: https://web.williams.edu/Mathematics/lg5/A37W12/Chain.pdf\n",
    "\n",
    "the neural network is formed in a layer-by-layer composition of functions. For instance, assuming the input is x0, and the first layer is f(x;W0,b0), we have\n",
    "\n",
    "$x0 - f0 \\rightarrow x1=f0(x0;\\theta_0) \\rightarrow x2=f1(x1;\\theta_1) \\rightarrow x3=f2(x2;\\theta_2) \\rightarrow ...$\n",
    "\n",
    "Hence using chain rule, we have:\n",
    "$$\\frac{\\partial x_{i}}{\\partial x_{i-2}}=\\frac{\\partial f_{i-1}(x_{i-1};\\theta_{i-1})}{\\partial x_{i-2}}\n",
    "=\n",
    "\\frac{\\partial f_{i-1}(f_{i-2}(x_{i-2};\\theta_{i-2});\\theta_{i-1})}{\\partial x_{i-2}}\n",
    "=\\\\\n",
    "\\frac{\\partial f_{i-1}(f_{i-2}(x_{i-2};\\theta_{i-2});\\theta_{i-1})}{\\partial f_{i-2}(x_{i-2};\\theta_{i-2})}\\frac{\\partial f_{i-2}(x_{i-2};\\theta_{i-2})}{\\partial x_{i-2}}\n",
    "+\n",
    "\\frac{\\partial f_{i-1}(f_{i-2}(x_{i-2};\\theta_{i-2});\\theta_{i-1})}{\\partial\\theta_{i-1}}\\frac{\\partial\\theta_{i-1}}{\\partial x_{i-2}}\\\\\n",
    "=\\frac{\\partial f_{i-1}(x_{i-1};\\theta_{i-1})}{\\partial x_{i-1}}\\frac{\\partial x_{i-1}}{\\partial x_{i-2}}\n",
    "+\n",
    "\\frac{\\partial f_{i-1}(x_{i-1};\\theta_{i-1})}{\\partial\\theta_{i-1}}\\frac{\\partial\\theta_{i-1}}{\\partial x_{i-2}}\\\\\n",
    "=\n",
    "\\frac{\\partial f_{i-1}(x_{i-1};\\theta_{i-1})}{\\partial x_{i-1}}\\frac{\\partial x_{i-1}}{\\partial x_{i-2}}\\\\\n",
    "$$\n",
    "\n",
    "Another way to see it is that if $y=f(a_1,a_2,...,a_N)$ is only a function of $a_1$, $\\dots$, $a_N$, then we have\n",
    "$$\\frac{\\partial y}{\\partial b}=\\sum_i\\frac{\\partial y}{\\partial a_i}\\frac{\\partial a_i}{\\partial b}$$\n",
    "\n",
    "Hence we have\n",
    "$$\\frac{\\partial x_N}{\\partial \\theta_i} = \\frac{\\partial x_N}{\\partial x_{i+1}}\\frac{\\partial x_{i+1}}{\\partial \\theta_i},$$\n",
    "$$\\frac{\\partial x_N}{\\partial x_i} = \\frac{\\partial x_N}{\\partial x_{i+1}}\\frac{\\partial x_{i+1}}{\\partial x_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752a994",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer:\n",
    "    def __init__(self, in_size, out_size):\n",
    "        \"\"\"\n",
    "        define the neural network layer by the input size and the output size.\n",
    "        TODO: For generealization, we can use a general large parameter matrix theta as a\n",
    "        replacement for W and b.\n",
    "        \"\"\"\n",
    "        W = np.random.random((out_size, in_size)) # weight shape: out_size x in_size\n",
    "        b = np.random.random((out_size))\n",
    "        self.out_size = out_size\n",
    "        self.in_size = in_size\n",
    "        self.param = [W, b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        given an input x of size m x n, where m denotes the batch size, and n is the input size.\n",
    "        compute an output by:\n",
    "        W x + b\n",
    "        of shape m x out_size\n",
    "        \"\"\"\n",
    "        W, b = self.param\n",
    "        return W.dot(x.T).T + b.reshape((1,-1))\n",
    "    \n",
    "    def compute_gradient(self, x):\n",
    "        \"\"\"\n",
    "        compute the jacobian/gradient of the output w.r.t. the weight with\n",
    "        the input x of shape: m x n\n",
    "        grad shape: m x out_size x out_size x in_size        \n",
    "        output: (W_grad, b_grad)\n",
    "        dy_i/dW_{i,j} = x_j\n",
    "        dy_i/db_{i} = 1\n",
    "        dy_i/dx_j = W[i,j]\n",
    "        \"\"\"\n",
    "        W, b =  self.param\n",
    "        W_grad = np.zeros((len(x), self.out_size, self.out_size, self.in_size))\n",
    "        b_grad = np.zeros((len(x), self.out_size, self.out_size))\n",
    "        # use broadcasting for diagonal slices\n",
    "        idx = np.arange(self.out_size)\n",
    "        W_grad[:,idx,idx,:] += x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        b_grad = b_grad + np.eye(self.out_size).reshape((1, self.out_size, self.out_size))\n",
    "        x_grad = np.array(W)\n",
    "        return (W_grad, b_grad), x_grad\n",
    "\n",
    "    def gradient_descent_step(self, param_grad, lr=1e-3):\n",
    "        \"\"\"\n",
    "        given parameter gradient (same size as the weight), perform one gradient descent step.\n",
    "        \"\"\"\n",
    "        self.param[0] -= lr * param_grad[0]\n",
    "        self.param[1] -= lr * param_grad[1]\n",
    "\n",
    "\n",
    "    def compute_gradient_numerical(self, x, eps=1e-4):\n",
    "        \"\"\"\n",
    "        numerical way to compute the gradient for verification of the correctness of the implementation\n",
    "        dy_i/dW_{j,k} = dy(W+eps)-d(W-eps)/2eps\n",
    "        \"\"\"\n",
    "        W, b =  self.param\n",
    "        W = np.array(W)\n",
    "        b = np.array(b)\n",
    "\n",
    "        W_grad = np.zeros((len(x), self.out_size, self.out_size, self.in_size))\n",
    "        b_grad = np.zeros((len(x), self.out_size, self.out_size))\n",
    "        # compute the gradient by perturbing each element\n",
    "        for j in range(len(W)):\n",
    "            for k in range(len(W[j])):\n",
    "                # perturbing W[j,k]\n",
    "                W[j,k] += eps\n",
    "                y_plus = W.dot(x.T).T + b.reshape((1,-1))\n",
    "                W[j,k] -= 2*eps\n",
    "                y_minus = W.dot(x.T).T + b.reshape((1,-1))\n",
    "                W_grad[:,:,j,k] = (y_plus - y_minus) / (2*eps)\n",
    "                W[j,k] += eps  # restore\n",
    "\n",
    "        for j in range(self.out_size):\n",
    "            b[j] += eps\n",
    "            y_plus = W.dot(x.T).T + b.reshape((1,-1))\n",
    "            b[j] -= 2*eps\n",
    "            y_minus = W.dot(x.T).T + b.reshape((1,-1))\n",
    "            b_grad[:,:,j] = (y_plus - y_minus) / (2*eps)\n",
    "            b[j] += eps  # restore\n",
    "        \n",
    "        # verify derivative w.r.t. x\n",
    "        x_temp = np.array(x)\n",
    "        x_grad = np.zeros((x.shape[0], self.out_size, self.in_size))\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[i])):\n",
    "                x_temp[i,j] += eps\n",
    "                y_plus = W.dot(x_temp.T).T + b.reshape((1,-1))\n",
    "                x_temp[i,j] -= 2*eps\n",
    "                y_minus = W.dot(x_temp.T).T + b.reshape((1,-1))\n",
    "                x_grad[i,:,j] = (y_plus - y_minus)[i] / (2*eps)\n",
    "                x_temp[i,j] += eps  # restore\n",
    "        return (W_grad, b_grad), x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4867c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: \n",
      "[[0.10058577 0.32548229 0.02976696 0.97243409 0.06908887]\n",
      " [0.1766955  0.07609467 0.76817591 0.16623673 0.07035571]\n",
      " [0.73648811 0.76421214 0.47227127 0.99681991 0.32714391]\n",
      " [0.74076242 0.96439735 0.63214915 0.23634061 0.78605033]\n",
      " [0.85923821 0.33299582 0.12543781 0.51797779 0.84888386]\n",
      " [0.76250184 0.00937683 0.21214427 0.65509099 0.8247184 ]\n",
      " [0.10338014 0.80944271 0.26646695 0.80830126 0.74270154]]\n",
      "b: \n",
      "[0.89189944 0.29548965 0.96117748 0.49368736 0.2612125  0.44230609\n",
      " 0.41797309]\n",
      "x:  [[0.0135492  0.01462823 0.42831771 0.77076787 0.33118674]\n",
      " [0.72546542 0.44703816 0.0339077  0.44777169 0.27559822]\n",
      " [0.94686325 0.44903629 0.61489482 0.87060749 0.5250232 ]\n",
      " [0.79130017 0.83218478 0.27380409 0.93883728 0.44198019]\n",
      " [0.81551011 0.27636687 0.86062234 0.48192744 0.56452607]\n",
      " [0.59472073 0.23820507 0.76165278 0.54484894 0.91110554]\n",
      " [0.29153494 0.2879805  0.93579459 0.30564863 0.77387223]\n",
      " [0.69341342 0.20472357 0.68202462 0.68492385 0.69714438]\n",
      " [0.36087541 0.4728263  0.95531433 0.03334487 0.73585512]\n",
      " [0.31258158 0.10001382 0.47726459 0.68685725 0.89550951]]\n",
      "y:  [[1.68317551 0.77945102 2.06128    1.23108538 1.01183257 1.32169857\n",
      "  1.41433258]\n",
      " [1.56585249 0.57756644 2.38962777 1.80610272 1.50356192 1.52748205\n",
      "  1.43048051]\n",
      " [2.03447898 1.15097811 3.33168417 2.63529727 2.1980909  2.30227142\n",
      "  2.13682691]\n",
      " [2.19399799 0.89612844 3.38967899 2.62479866 2.11407399 2.09109619\n",
      "  2.33346619]\n",
      " [1.59714393 1.2415577  2.84451724 2.46600069 1.89075832 2.03058296\n",
      "  1.76412814]\n",
      " [1.64470044 1.15845897 2.78210706 2.49038136 2.00272306 2.16792676\n",
      "  1.99230563]\n",
      " [1.39350112 1.19302757 2.39576081 2.25947456 1.54023763 1.70428015\n",
      "  1.75238632]\n",
      " [1.76279088 1.12041353 2.86123335 2.34578399 1.95731352 2.14127761\n",
      "  1.9085023 ]\n",
      " [1.19379684 1.18639849 2.31343439 2.40720377 1.49049959 1.5532895\n",
      "  1.66603936]\n",
      " [1.69989329 0.9021402  2.47085369 1.98963931 1.73892612 1.97133405\n",
      "  1.87870239]]\n",
      "y.shape:  (10, 7)\n",
      "yi.shape:  (7,)\n",
      "yi: \n",
      "[1.68317551 0.77945102 2.06128    1.23108538 1.01183257 1.32169857\n",
      " 1.41433258]\n",
      "y[i]: \n",
      "[1.68317551 0.77945102 2.06128    1.23108538 1.01183257 1.32169857\n",
      " 1.41433258]\n",
      "yi.shape:  (7,)\n",
      "yi: \n",
      "[1.56585249 0.57756644 2.38962777 1.80610272 1.50356192 1.52748205\n",
      " 1.43048051]\n",
      "y[i]: \n",
      "[1.56585249 0.57756644 2.38962777 1.80610272 1.50356192 1.52748205\n",
      " 1.43048051]\n",
      "yi.shape:  (7,)\n",
      "yi: \n",
      "[2.03447898 1.15097811 3.33168417 2.63529727 2.1980909  2.30227142\n",
      " 2.13682691]\n",
      "y[i]: \n",
      "[2.03447898 1.15097811 3.33168417 2.63529727 2.1980909  2.30227142\n",
      " 2.13682691]\n",
      "yi.shape:  (7,)\n",
      "yi: \n",
      "[2.19399799 0.89612844 3.38967899 2.62479866 2.11407399 2.09109619\n",
      " 2.33346619]\n",
      "y[i]: \n",
      "[2.19399799 0.89612844 3.38967899 2.62479866 2.11407399 2.09109619\n",
      " 2.33346619]\n",
      "yi.shape:  (7,)\n",
      "yi: \n",
      "[1.59714393 1.2415577  2.84451724 2.46600069 1.89075832 2.03058296\n",
      " 1.76412814]\n",
      "y[i]: \n",
      "[1.59714393 1.2415577  2.84451724 2.46600069 1.89075832 2.03058296\n",
      " 1.76412814]\n",
      "yi.shape:  (7,)\n",
      "yi: \n",
      "[1.64470044 1.15845897 2.78210706 2.49038136 2.00272306 2.16792676\n",
      " 1.99230563]\n",
      "y[i]: \n",
      "[1.64470044 1.15845897 2.78210706 2.49038136 2.00272306 2.16792676\n",
      " 1.99230563]\n",
      "yi.shape:  (7,)\n",
      "yi: \n",
      "[1.39350112 1.19302757 2.39576081 2.25947456 1.54023763 1.70428015\n",
      " 1.75238632]\n",
      "y[i]: \n",
      "[1.39350112 1.19302757 2.39576081 2.25947456 1.54023763 1.70428015\n",
      " 1.75238632]\n",
      "yi.shape:  (7,)\n",
      "yi: \n",
      "[1.76279088 1.12041353 2.86123335 2.34578399 1.95731352 2.14127761\n",
      " 1.9085023 ]\n",
      "y[i]: \n",
      "[1.76279088 1.12041353 2.86123335 2.34578399 1.95731352 2.14127761\n",
      " 1.9085023 ]\n",
      "yi.shape:  (7,)\n",
      "yi: \n",
      "[1.19379684 1.18639849 2.31343439 2.40720377 1.49049959 1.5532895\n",
      " 1.66603936]\n",
      "y[i]: \n",
      "[1.19379684 1.18639849 2.31343439 2.40720377 1.49049959 1.5532895\n",
      " 1.66603936]\n",
      "yi.shape:  (7,)\n",
      "yi: \n",
      "[1.69989329 0.9021402  2.47085369 1.98963931 1.73892612 1.97133405\n",
      " 1.87870239]\n",
      "y[i]: \n",
      "[1.69989329 0.9021402  2.47085369 1.98963931 1.73892612 1.97133405\n",
      " 1.87870239]\n",
      "x_grad: \n",
      "[[0.10058577 0.32548229 0.02976696 0.97243409 0.06908887]\n",
      " [0.1766955  0.07609467 0.76817591 0.16623673 0.07035571]\n",
      " [0.73648811 0.76421214 0.47227127 0.99681991 0.32714391]\n",
      " [0.74076242 0.96439735 0.63214915 0.23634061 0.78605033]\n",
      " [0.85923821 0.33299582 0.12543781 0.51797779 0.84888386]\n",
      " [0.76250184 0.00937683 0.21214427 0.65509099 0.8247184 ]\n",
      " [0.10338014 0.80944271 0.26646695 0.80830126 0.74270154]]\n",
      "W_grad: \n",
      "[[[[0.0135492  0.01462823 0.42831771 0.77076787 0.33118674]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.0135492  0.01462823 0.42831771 0.77076787 0.33118674]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.0135492  0.01462823 0.42831771 0.77076787 0.33118674]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.0135492  0.01462823 0.42831771 0.77076787 0.33118674]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.0135492  0.01462823 0.42831771 0.77076787 0.33118674]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.0135492  0.01462823 0.42831771 0.77076787 0.33118674]]]\n",
      "\n",
      "\n",
      " [[[0.72546542 0.44703816 0.0339077  0.44777169 0.27559822]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.72546542 0.44703816 0.0339077  0.44777169 0.27559822]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.72546542 0.44703816 0.0339077  0.44777169 0.27559822]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.72546542 0.44703816 0.0339077  0.44777169 0.27559822]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.72546542 0.44703816 0.0339077  0.44777169 0.27559822]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.72546542 0.44703816 0.0339077  0.44777169 0.27559822]]]\n",
      "\n",
      "\n",
      " [[[0.94686325 0.44903629 0.61489482 0.87060749 0.5250232 ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.94686325 0.44903629 0.61489482 0.87060749 0.5250232 ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.94686325 0.44903629 0.61489482 0.87060749 0.5250232 ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.94686325 0.44903629 0.61489482 0.87060749 0.5250232 ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.94686325 0.44903629 0.61489482 0.87060749 0.5250232 ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.94686325 0.44903629 0.61489482 0.87060749 0.5250232 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.69341342 0.20472357 0.68202462 0.68492385 0.69714438]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.69341342 0.20472357 0.68202462 0.68492385 0.69714438]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.69341342 0.20472357 0.68202462 0.68492385 0.69714438]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.69341342 0.20472357 0.68202462 0.68492385 0.69714438]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.69341342 0.20472357 0.68202462 0.68492385 0.69714438]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.69341342 0.20472357 0.68202462 0.68492385 0.69714438]]]\n",
      "\n",
      "\n",
      " [[[0.36087541 0.4728263  0.95531433 0.03334487 0.73585512]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.36087541 0.4728263  0.95531433 0.03334487 0.73585512]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.36087541 0.4728263  0.95531433 0.03334487 0.73585512]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.36087541 0.4728263  0.95531433 0.03334487 0.73585512]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.36087541 0.4728263  0.95531433 0.03334487 0.73585512]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.36087541 0.4728263  0.95531433 0.03334487 0.73585512]]]\n",
      "\n",
      "\n",
      " [[[0.31258158 0.10001382 0.47726459 0.68685725 0.89550951]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.31258158 0.10001382 0.47726459 0.68685725 0.89550951]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.31258158 0.10001382 0.47726459 0.68685725 0.89550951]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.31258158 0.10001382 0.47726459 0.68685725 0.89550951]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.31258158 0.10001382 0.47726459 0.68685725 0.89550951]\n",
      "   [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         0.         0.        ]\n",
      "   [0.31258158 0.10001382 0.47726459 0.68685725 0.89550951]]]]\n",
      "b_grad: \n",
      "[[[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]]\n",
      "W_grad_diff:  1.640514723682e-11\n",
      "b_grad_diff:  7.520008995151743e-12\n",
      "x_grad_diff:  1.932395965844112e-11\n"
     ]
    }
   ],
   "source": [
    "# test code for verifying FCLayer\n",
    "in_size = 5\n",
    "out_size = 7\n",
    "fcLayer = FCLayer(in_size, out_size)\n",
    "W, b = fcLayer.param\n",
    "print('W: ')\n",
    "print(W)\n",
    "print('b: ')\n",
    "print(b)\n",
    "\n",
    "\n",
    "x = np.random.random((10, in_size))\n",
    "print('x: ', x)\n",
    "y = fcLayer.forward(x)\n",
    "print('y: ', y)\n",
    "print('y.shape: ', y.shape)\n",
    "# verify if the output is correct using for-loop (for verifying against vectorization)\n",
    "for i in range(len(x)):\n",
    "    W, b = fcLayer.param\n",
    "    yi = W.dot(x[i]) + b\n",
    "    print('yi.shape: ', yi.shape)\n",
    "    print('yi: ')\n",
    "    print(yi)\n",
    "    print('y[i]: ')\n",
    "    print(y[i])\n",
    "    assert(np.isclose(y[i], yi).all())\n",
    "\n",
    "grad, x_grad = fcLayer.compute_gradient(x)\n",
    "W_grad, b_grad = grad\n",
    "print('x_grad: ')\n",
    "print(x_grad)\n",
    "\n",
    "print('W_grad: ')\n",
    "print(W_grad)\n",
    "print('b_grad: ')\n",
    "print(b_grad)\n",
    "\n",
    "grad_num, x_grad_num = fcLayer.compute_gradient_numerical(x)\n",
    "W_grad_num, b_grad_num = grad_num\n",
    "# compute the gradient using numerical method\n",
    "W_grad_diff = np.linalg.norm(W_grad_num - W_grad)\n",
    "b_grad_diff = np.linalg.norm(b_grad_num - b_grad)\n",
    "x_grad_diff = np.linalg.norm(x_grad_num - x_grad)\n",
    "print('W_grad_diff: ', W_grad_diff)\n",
    "print('b_grad_diff: ', b_grad_diff)\n",
    "print('x_grad_diff: ', x_grad_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self, in_size):\n",
    "        \"\"\"\n",
    "        define the neural network layer by the input size and the output size.\n",
    "        TODO: For generealization, we can use a general large parameter matrix theta as a\n",
    "        replacement for W and b.\n",
    "        \"\"\"\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.param = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        g(x[i]) = 0 if x[i] <= 0; otherwise x[i]\n",
    "        \"\"\"\n",
    "        res = np.array(x)\n",
    "        res[res<=0] = 0.0\n",
    "        return res\n",
    "    \n",
    "    def compute_gradient(self, x):\n",
    "        \"\"\"\n",
    "        dy_i/dx_j = 0 if i != j\n",
    "        dy_i/dx_i = 0 if x_i <= 0\n",
    "        dy_i/dx_i = 1 if x_i > 0\n",
    "        the input x of shape: m x n\n",
    "        grad w.r.t. x shape: m x n x n\n",
    "        \"\"\"\n",
    "        x_grad = np.zeros((x.shape[0], x.shape[1], x.shape[1]))\n",
    "        # use broadcasting for diagonal slices\n",
    "        idx = np.arange(self.in_size)\n",
    "        x_grad[:,idx,idx] = (x > 0).astype(float)        \n",
    "        return (), x_grad\n",
    "\n",
    "    def gradient_descent_step(self, param_grad, lr=1e-3):\n",
    "        \"\"\"\n",
    "        given parameter gradient (same size as the weight), perform one gradient descent step.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def compute_gradient_numerical(self, x, eps=1e-4):\n",
    "        \"\"\"\n",
    "        (y(x+eps)-y(x-eps))/2eps\n",
    "        shape: m x n x n\n",
    "        \"\"\"\n",
    "        x_grad = np.zeros((x.shape[0], x.shape[1], x.shape[1]))\n",
    "        x_temp = np.array(x)\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[i])):\n",
    "                x_temp[i,j] += eps\n",
    "                y_plus = self.forward(x_temp)\n",
    "                x_temp[i,j] -= 2*eps\n",
    "                y_minus = self.forward(x_temp)\n",
    "                x_grad[i,:,j] = (y_plus - y_minus)[i] / (2*eps)\n",
    "                x_temp[i,j] += eps # restore\n",
    "        return (), x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b655af29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [[ 0.13228786 -0.31128811  0.05438585 -0.11856732 -0.44988296]\n",
      " [-0.32932097  0.17271092  0.31953201 -0.36058933  0.42877293]\n",
      " [ 0.14822497  0.24614302 -0.19479945 -0.49159779  0.25312682]\n",
      " [ 0.02473314  0.31214884  0.07660131 -0.14586548 -0.28715797]\n",
      " [-0.03218097  0.48306264 -0.48967294  0.17092911  0.40598261]\n",
      " [ 0.23741837  0.40755568 -0.31595947  0.03794493 -0.43146524]\n",
      " [-0.36902465  0.1736176   0.41128677 -0.30725618 -0.09007552]\n",
      " [-0.16019618 -0.1410945   0.33636295 -0.0977237   0.4743587 ]\n",
      " [-0.00526242  0.22304308  0.39249721 -0.11239164  0.30209213]\n",
      " [ 0.06049701  0.22630192 -0.26048769 -0.38813916  0.18775007]]\n",
      "y:  [[0.13228786 0.         0.05438585 0.         0.        ]\n",
      " [0.         0.17271092 0.31953201 0.         0.42877293]\n",
      " [0.14822497 0.24614302 0.         0.         0.25312682]\n",
      " [0.02473314 0.31214884 0.07660131 0.         0.        ]\n",
      " [0.         0.48306264 0.         0.17092911 0.40598261]\n",
      " [0.23741837 0.40755568 0.         0.03794493 0.        ]\n",
      " [0.         0.1736176  0.41128677 0.         0.        ]\n",
      " [0.         0.         0.33636295 0.         0.4743587 ]\n",
      " [0.         0.22304308 0.39249721 0.         0.30209213]\n",
      " [0.06049701 0.22630192 0.         0.         0.18775007]]\n",
      "y.shape:  (10, 5)\n",
      "x_grad: \n",
      "[[[1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1.]]]\n",
      "W_grad_diff:  1.640514723682e-11\n",
      "b_grad_diff:  7.520008995151743e-12\n",
      "x_grad_diff:  3.9335551153712515e-13\n"
     ]
    }
   ],
   "source": [
    "# test code for verifying ReLU layer\n",
    "in_size = 5\n",
    "relu = ReLU(in_size)\n",
    "\n",
    "x = np.random.random((10, in_size)) - 0.5\n",
    "print('x: ', x)\n",
    "y = relu.forward(x)\n",
    "print('y: ', y)\n",
    "print('y.shape: ', y.shape)\n",
    "# verify if the output is correct using for-loop (for verifying against vectorization)\n",
    "\n",
    "grad, x_grad = relu.compute_gradient(x)\n",
    "print('x_grad: ')\n",
    "print(x_grad)\n",
    "\n",
    "grad_num, x_grad_num = relu.compute_gradient_numerical(x)\n",
    "# compute the gradient using numerical method\n",
    "W_grad_diff = np.linalg.norm(W_grad_num - W_grad)\n",
    "b_grad_diff = np.linalg.norm(b_grad_num - b_grad)\n",
    "x_grad_diff = np.linalg.norm(x_grad_num - x_grad)\n",
    "print('W_grad_diff: ', W_grad_diff)\n",
    "print('b_grad_diff: ', b_grad_diff)\n",
    "print('x_grad_diff: ', x_grad_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b901f1b",
   "metadata": {},
   "source": [
    "#### define a neural network model\n",
    "##### implement the forward propagation\n",
    "##### implement the back-propagation\n",
    "store the values of each layer (input and output), chaining the back-propagated jacobian/gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369a79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwrokModel:\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        neural network is defined to be a list of layers from the first layer to the last.\n",
    "        each layer can be (in our simplified case) either a fully-connected layer or ReLU layer.\n",
    "        \"\"\"\n",
    "        input_size = layers[0].in_size\n",
    "        output_size = layers[-1].out_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        given an input x of size mxn, forward propagate the model.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i].forward(x)\n",
    "        return x\n",
    "\n",
    "    def compute_gradient(self, x):\n",
    "        \"\"\"\n",
    "        given an input x of size mxn, back-propagate the model to compute the gradients for each parameters.\n",
    "        - forward once to get the values for each layer (input, output)\n",
    "        - keep track of d(output)/dxi for each layer\n",
    "\n",
    "        dout/dxi = dout/dx_{i+1} * dx{i+1}/dxi\n",
    "        dout/dWi = dout/dx_{i+1} * dx{i+1}/dWi\n",
    "        \n",
    "        Caveat:\n",
    "        dout/dxi is of shape m x n_out x n_in\n",
    "        dout/dWi is of shape m x n_out x n_out x n_in  (which can use np.matmul to get the result)\n",
    "        \"\"\"\n",
    "        inputs = [x]  # store inputs of each layer\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i].forward(x)\n",
    "            inputs.append(x)\n",
    "        # initialize the jacobian/gradient\n",
    "        dout_dx = np.zeros((len(x),self.output_size, self.output_size)) + np.eye(self.output_size).reshape((1, self.output_size, self.output_size))\n",
    "        dout_dparams = []\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            # compute the commulative value\n",
    "            param_grad, x_grad = self.layers[i].compute_gradient(inputs[i])\n",
    "            # dout_dparam = [np.matmul(dout_dx, param_grad[k]) for k in range(len(param_grad))]\n",
    "            # Method 1: flatten the last few dimensions, and then reshape them\n",
    "            # for k in range(len(param_grad)):\n",
    "            #     # dout_dx of shape: N x n_out x n_out_i\n",
    "            #     # param_grad of shape: N x n_out_i x ...\n",
    "            #     # for easy compute, flatten the last few dims of the param, conduct matmul, and then reshape\n",
    "            #     param_grad_k = param_grad[k].reshape((param_grad.shape[0],param_grad.shape[1],-1))\n",
    "            #     param_grad_k = np.matmul(dout_dx, param_grad_k)\n",
    "            #     param_grad_k = param_grad_k.reshape(list(dout_dx.shape)[:2] + list(param_grad[k].shape)[2:])\n",
    "            #     dout_dparam.append(param_grad_k)\n",
    "            # Method 2: using einsum\n",
    "        #     # dout_dx of shape: N x n_out x n_out_i\n",
    "        #     # param_grad of shape: N x n_out_i x ...\n",
    "            dout_dparam = [np.einsum('ijk...,ik...->ij...', dout_dx, param_grad[k]) for k in range(len(param_grad))]\n",
    "\n",
    "            dout_dparams.append(dout_dparam)\n",
    "            dout_dx = np.matmul(dout_dx, x_grad)\n",
    "            # # DEBUG\n",
    "            # print(f'layer {i} parameter size:')\n",
    "            # print(f'input size: {self.layers[i].in_size}, output size: {self.layers[i].out_size}')\n",
    "            # for param in dout_dparam:\n",
    "            #     print(param.shape)\n",
    "            # print('dout_dx shape: ', dout_dx.shape)\n",
    "\n",
    "        dout_dparams = dout_dparams[::-1]\n",
    "        return dout_dparams\n",
    "\n",
    "    def gradient_descent_step(self, dout_dparams, lr=1e-3):\n",
    "        \"\"\"\n",
    "        given parameter gradient (same size as the weight), perform one gradient descent step.\n",
    "        \"\"\"\n",
    "        for i in range(len(dout_dparams)):\n",
    "            self.layers[i].gradient_descent_step(dout_dparams[i], lr)\n",
    "\n",
    "    def compute_gradient_numerical(self, x, eps=1e-4):\n",
    "        \"\"\"\n",
    "        perturb each weight value of the neural networks\n",
    "        compute new forwarded output value, and compute doutput/dw\n",
    "        shape: N x N_output x weight_shape\n",
    "        \"\"\"\n",
    "        dout_dparams = []\n",
    "        # loop through the layers to purterb each layer's weights\n",
    "        for i in range(len(self.layers)):\n",
    "            params = self.layers[i].param\n",
    "            param_grad = []\n",
    "            for param in params:\n",
    "                # shape of the gradient: N x N_output x weight_shape\n",
    "                grad = np.zeros([len(x), self.output_size] + list(param.shape))\n",
    "                # update the weight, and compute\n",
    "                with np.nditer(param, op_flags=['readwrite'], flags=['multi_index']) as it:\n",
    "                    for element in it:\n",
    "                        current_idx = it.multi_index\n",
    "                        prev_val = np.array(element)\n",
    "                        element[...] += eps\n",
    "                        y_plus = self.forward(x)\n",
    "                        element[...] -= 2*eps\n",
    "                        y_minus = self.forward(x)\n",
    "                        idx_tuple = tuple([slice(None), slice(None)] + list(current_idx))\n",
    "                        grad[idx_tuple] = (y_plus-y_minus)/(2*eps)\n",
    "                        element[...] = prev_val\n",
    "                param_grad.append(grad)\n",
    "            dout_dparams.append(param_grad)\n",
    "        return dout_dparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d5d6878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: \n",
      "[[0.65969539 0.18502667 0.01199677]]\n",
      "layer 0 parameter size:\n",
      "input size: 2, output size: 3\n",
      "(1, 3, 3, 2)\n",
      "(1, 3, 3)\n",
      "dout_dx shape:  (1, 3, 2)\n",
      "param_grad: \n",
      "(array([[[[ 0.28592869, -0.48186442],\n",
      "         [ 0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ]],\n",
      "\n",
      "        [[ 0.        ,  0.        ],\n",
      "         [ 0.28592869, -0.48186442],\n",
      "         [ 0.        ,  0.        ]],\n",
      "\n",
      "        [[ 0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ],\n",
      "         [ 0.28592869, -0.48186442]]]]), array([[[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]]]))\n",
      "nn_grad: \n",
      "[[array([[[[ 0.28592869, -0.48186442],\n",
      "         [ 0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ]],\n",
      "\n",
      "        [[ 0.        ,  0.        ],\n",
      "         [ 0.28592869, -0.48186442],\n",
      "         [ 0.        ,  0.        ]],\n",
      "\n",
      "        [[ 0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ],\n",
      "         [ 0.28592869, -0.48186442]]]]), array([[[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]]])]]\n",
      "numerical: \n",
      "[[array([[[[ 0.28592869, -0.48186442],\n",
      "         [ 0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ]],\n",
      "\n",
      "        [[ 0.        ,  0.        ],\n",
      "         [ 0.28592869, -0.48186442],\n",
      "         [ 0.        ,  0.        ]],\n",
      "\n",
      "        [[ 0.        ,  0.        ],\n",
      "         [ 0.        ,  0.        ],\n",
      "         [ 0.28592869, -0.48186442]]]]), array([[[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]]])]]\n"
     ]
    }
   ],
   "source": [
    "# test the neural network model\n",
    "input_size = 2\n",
    "out_size1 = 3\n",
    "fcl1 = FCLayer(input_size, out_size1)\n",
    "\n",
    "layers = [fcl1]\n",
    "nn_model = NeuralNetwrokModel(layers)\n",
    "\n",
    "x = np.random.random((1, input_size)) - 0.5\n",
    "y = nn_model.forward(x)\n",
    "print('y: ')\n",
    "print(y)\n",
    "\n",
    "dout_dparams = nn_model.compute_gradient(x)\n",
    "param_grad, x_grad = fcl1.compute_gradient(x)\n",
    "print('param_grad: ')\n",
    "print(param_grad)\n",
    "print('nn_grad: ')\n",
    "print(dout_dparams)\n",
    "\n",
    "dout_dparams_num = nn_model.compute_gradient_numerical(x)\n",
    "print('numerical: ')\n",
    "print(dout_dparams_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4509a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: \n",
      "[[ 1.44395276  0.91942089  1.48690137  1.37619499]\n",
      " [ 1.31352228  0.78541616  1.32538177  1.23555411]\n",
      " [ 0.87515579  0.31937988  0.82117784  0.79687416]\n",
      " [ 0.98966095  0.46504691  0.89379667  0.85948232]\n",
      " [ 1.14550553  0.63922422  1.05207478  0.99698778]\n",
      " [ 0.35567625 -0.21985698  0.19150726  0.24872038]\n",
      " [ 1.73230421  1.23313343  1.80088409  1.64920243]\n",
      " [ 1.43851357  0.92954648  1.4413749   1.33620377]\n",
      " [ 1.82938258  1.34852002  1.88247368  1.71989716]\n",
      " [ 1.41471609  0.89143288  1.44563571  1.34021789]]\n",
      "layer 1 parameter size:\n",
      "input size: 3, output size: 4\n",
      "(10, 4, 4, 3)\n",
      "(10, 4, 4)\n",
      "dout_dx shape:  (10, 4, 3)\n",
      "layer 0 parameter size:\n",
      "input size: 2, output size: 3\n",
      "(10, 4, 3, 2)\n",
      "(10, 4, 3)\n",
      "dout_dx shape:  (10, 4, 2)\n",
      "param gradient vs numerical gradient:  9.608429001887627e-12\n",
      "param gradient vs numerical gradient:  5.943111187168264e-12\n",
      "param gradient vs numerical gradient:  6.9253730956772795e-12\n",
      "param gradient vs numerical gradient:  6.965493601656175e-13\n"
     ]
    }
   ],
   "source": [
    "# test the neural network model\n",
    "input_size = 2\n",
    "out_size1 = 3\n",
    "fcl1 = FCLayer(input_size, out_size1)\n",
    "out_size2 = 4\n",
    "fcl2 = FCLayer(out_size1, out_size2)\n",
    "\n",
    "layers = [fcl1, fcl2]\n",
    "nn_model = NeuralNetwrokModel(layers)\n",
    "\n",
    "\n",
    "x = np.random.random((10, input_size)) - 0.5\n",
    "y = nn_model.forward(x)\n",
    "print('y: ')\n",
    "print(y)\n",
    "\n",
    "dout_dparams = nn_model.compute_gradient(x)\n",
    "# print('param_grad: ')\n",
    "# print(param_grad)\n",
    "\n",
    "dout_dparams_num = nn_model.compute_gradient_numerical(x)\n",
    "# print('numerical: ')\n",
    "# print(dout_dparams_num)\n",
    "\n",
    "# compare the two params\n",
    "for i in range(len(dout_dparams)):\n",
    "    dout_dparam = dout_dparams[i]\n",
    "    dout_dparam_num = dout_dparams_num[i]\n",
    "    for j in range(len(dout_dparam)):\n",
    "        param_grad = dout_dparam[j]\n",
    "        param_grad_num = dout_dparam_num[j]\n",
    "        print('param gradient vs numerical gradient: ', np.linalg.norm(param_grad-param_grad_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff129c",
   "metadata": {},
   "source": [
    "#### define the loss and define the machine learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed9910",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- define losses as classes for computation of gradient\n",
    "- define machine learning problem as a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    pass\n",
    "    # def __init__(self, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08eed8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1198969800.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mclass MSELoss:\u001b[39m\n                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class MSELoss:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99da23d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
